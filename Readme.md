# ğŸ”§ Interview Helper â€” Phase 1

**Real-time Interview Assistant with Live Transcription, Question Detection & GPT-4o-mini Answers**

Interview Helper is a Python-based tool designed to assist during **live technical interviews**.
It listens to system audio (WASAPI loopback), transcribes the conversation using **Whisper**, detects questions in real-time, and instantly generates **bullet-point answers** tailored to:

* Your **resume**
* The **current job description**
* The **role you are interviewing for**

Phase 1 focuses on the **core pipeline**, delivering stable, accurate, real-time Q&A assistance.

---

## âœ¨ Features (Phase 1)

### ğŸ¤ 1. Real-Time System Audio Capture

* Captures **desktop audio only** (no mic required)
* Uses **WASAPI loopback** â†’ works with Zoom, Meet, Teams, YouTube practice videos
* Highly stable, low-latency stream

### ğŸ§  2. Whisper-Based Streaming STT

* Uses **faster-whisper**
* Sliding window transcription (10s window / 3s step)
* VAD gating
* Duplicate suppression
* Output is fast and high-quality

### â“ 3. Smart Question Detection

* Extracts the **real core question** the interviewer asks
* Removes filler, boilerplate (â€œletâ€™s begin with our first questionâ€¦â€)
* Deduplicates variations of the same question
* Filters out explanation statements (â€œX is part of Y, right?â€)

### ğŸ¤– 4. GPT-4o-mini Answer Engine

* Answers every detected question in **3â€“5 sharp bullet points**
* Uses:

  * Your **resume.md**
  * The **current_jd.md**
  * The **role** you're applying for
* Zero hallucinations thanks to strict system prompt constraints
* Intent-aware responses:

  * â€œTell me about yourselfâ€
  * â€œWhat have you studied?â€
  * â€œWhat is machine learning?â€
  * â€œStrengths/weaknessesâ€
  * â€œExperienceâ€
  * â€œData driftâ€
  * And generic MLOps/DevOps questions

### ğŸ“ 5. Automatic Q&A Logging

Every real question + generated answer is saved to:

```
/data/sessions/<timestamp>/qa_log.md
```

Useful for review, reflection, and improving your preparation.

---

# ğŸ“ Project Structure

```
interview_helper/
â”‚
â”œâ”€â”€ main.py                          # Main live pipeline
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml                # Audio + streaming configuration
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ audio_capture.py             # WASAPI loopback capture
â”‚   â”œâ”€â”€ stt_whisper_stream.py        # Whisper streaming pipeline
â”‚   â”œâ”€â”€ question_finder.py           # Smart question extraction & dedupe
â”‚   â”œâ”€â”€ answer_llm.py                # GPT-4o-mini answer engine
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ resume.md                    # Summary of your resume
â”‚   â”œâ”€â”€ current_jd.md                # JD for *this* interview (editable)
â”‚   â””â”€â”€ sessions/                    # STT & Q&A logs
â”‚
â””â”€â”€ README.md
```

---

# ğŸš€ Getting Started

## 1. Install dependencies

```bash
pip install -r requirements.txt
```

(If you donâ€™t have one yet, create it from your current environment.)

Ensure you also install:

```bash
pip install openai pyaudio numpy scipy faster-whisper pyyaml
```

## 2. Set your OpenAI API key

```
export OPENAI_API_KEY="your-key"
```

or on Windows:

```
setx OPENAI_API_KEY "your-key"
```

## 3. Edit your resume and JD summaries

`data/resume.md` â€” short 1â€“2 paragraph summary of your real resume
`data/current_jd.md` â€” paste/summarize the job description for the next call

These heavily shape your interview answers.

## 4. Run the system

```bash
python main.py
```

You should see:

```
[INFO] Capturing system audio...
[INFO] Starting live transcription + question finder...
ğŸ—£ï¸ <transcribed audio>
â“ Q: <detected question>
â¡ bullet answer 1
â¡ bullet answer 2
...
```

Press **Ctrl+C** to stop.

Your transcript and Q&A log will save automatically.

---

# ğŸ§© How It Works (Core Pipeline)

```
Desktop audio â†’ WASAPI loopback â†’ Whisper stream â†’ 
Sliding window â†’ Question Finder â†’ GPT-4o-mini â†’
Bullet-point answer â†’ Console + Q&A log
```

## Whisper Sliding Window

* 10s context
* 3s step
* Keeps Whisper accurate while staying near-real-time

## Question Finder

* Extracts the real core question
* Filters duplicates + explanations
* Normalizes phrasing
* Only fires when an **actual** question is detected

## GPT-4o-mini Answer Engine

* Uses role + resume + JD
* Intent-aware prompts
* 3â€“5 crisp bullets
* Zero rambling, zero paragraphs

---

# ğŸ—’ï¸ Output Example

```
â“ Q: tell me about yourself

â¡ Iâ€™m a Senior MLOps/DevOps engineer with experience across AWS and GCP.
â¡ I built end-to-end MLOps platforms for data ingestion, model training and deployment.
â¡ Strong in Terraform, Kubernetes, CI/CD and cloud security.
â¡ Experienced with predictive maintenance, document extraction, and ML observability.
â¡ I focus on productionizing ML systems at scale.
```

---

# ğŸ¯ Phase 1 Complete

Your system is now a functional **real-time interview assistant**:

* Live transcript
* Live question detection
* Live answer generation (LLM)
* Session logging
* Resume/JD-aware reasoning

Excellent foundation.

---

# â–¶ï¸ Phase 2 Preview (coming next)

You asked for:

1. **Auto-merged resume/JD summaries**
2. **Context memory of previous Q&A**
3. **A real UI overlay (always-on-top window)**
4. **End-of-session transcript export** (already added)

Phase 2 will add these without changing your Phase 1 pipeline.

---

If you'd like, I can generate:

* A **requirements.txt**
* A **sample JD summary**
* A **sample resume summary**
* Or even a GIF-style step-by-step demo sequence for the README.

Just say the word.
